{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW#5 (For Number 1,2, and 4 \"Bonus\")\n",
    "\n",
    "### Name : Tri Wanda Septian (106998406), Anggara Aji Saputra (106998412)\n",
    "\n",
    "- Data: \n",
    "    - [Google web graph Data Set] from Stanford Large Network (SNAP) Dataset Collection\n",
    "    - The data was released in 2002 by Google as a part of Google Programming Contest\n",
    "    - Available at: http://snap.stanford.edu/data/web-Google.html \n",
    "- Format:\n",
    "    - A text file containing around 5 million lines \n",
    "    - Each line is a directed edge representing hyperlinks between nodes (web pages)\n",
    "    \n",
    "    FromNodeId ToNodeId\n",
    "    \n",
    "    …\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "#### GraphFrames Overview\n",
    "\n",
    "GraphFrames is a package for Apache Spark which provides DataFrame-based Graphs. It provides high-level APIs in Scala, Java, and Python. It aims to provide both the functionality of GraphX and extended functionality taking advantage of Spark DataFrames. This extended functionality includes motif finding, DataFrame-based serialization, and highly expressive graph queries.\n",
    "\n",
    "#### What are GraphFrames?\n",
    "\n",
    "GraphX is to RDDs as GraphFrames are to DataFrames.\n",
    "\n",
    "GraphFrames represent graphs: vertices (e.g., users) and edges (e.g., relationships between users). If you are familiar with GraphX, then GraphFrames will be easy to learn. The key difference is that GraphFrames are based upon Spark DataFrames, rather than RDDs.\n",
    "\n",
    "GraphFrames also provide powerful tools for running queries and standard graph algorithms. With GraphFrames, you can easily search for patterns within graphs, find important vertices, and more. Refer to the User Guide for a full list of queries and algorithms.\n",
    "\n",
    "Will GraphFrames be part of Apache Spark?\n",
    "\n",
    "The GraphX component of Apache Spark has no DataFrames- or Dataset-based equivalent, so it is natural to ask this question. The current plan is to keep GraphFrames separate from core Apache Spark for the time being:\n",
    "\n",
    "- we are still considering making small adjustments to the API. The GraphFrames project will be considered for inclusion into Spark once we are confident that the current API addresses current and future needs.\n",
    "\n",
    "- some important features present in GraphX such as partitioning are missing. We would like to offer some equivalent operations before considering merging with the Spark project.\n",
    "\n",
    "- GraphFrames is used as a testbed for advanced, graph-specific optimizations into Spark’s Catalyst engine. Having them in a separate project accelerates the development cycle.\n",
    "\n",
    "That being said, GraphFrames follows the same code quality standards as Spark, and it is cross-compiled and published for a large number of Spark versions. It is easy for users to depend on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load graphframes\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = ('--jars graphframes-assembly-0.6.0-SNAPSHOT-spark2.3.jar pyspark-shell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import IntegerType\n",
    "from graphframes import *\n",
    "from pyspark.sql.functions import desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster(\"spark://sparklab1:7077\").setAppName(\"HW#5\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "sqlContext=SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.17.0.1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://sparklab1:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://sparklab1:7077 appName=PySparkShell>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = sc.textFile(\"/home/tri/Spark/dataset/web-Google-2.txt\",use_unicode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = raw_data.map(lambda x:x.split(\"\\t\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0', '11342'],\n",
       " ['0', '824020'],\n",
       " ['0', '867923'],\n",
       " ['0', '891835'],\n",
       " ['11342', '0']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = rdd.flatMap(lambda x: (x[0], x[1])).distinct()\n",
    "keylist = keys.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating GraphFrames\n",
    "\n",
    "Users can create GraphFrames from vertex and edge DataFrames.\n",
    "\n",
    "- Vertex DataFrame: A vertex DataFrame should contain a special column named “id” which specifies unique IDs for each vertex in the graph.\n",
    "- Edge DataFrame: An edge DataFrame should contain two special columns: “src” (source vertex ID of edge) and “dst” (destination vertex ID of edge).\n",
    "\n",
    "Both DataFrames can have arbitrary other columns. Those columns can represent vertex and edge attributes.\n",
    "\n",
    "A GraphFrame can also be constructed from a single DataFrame containing edge information. The vertices will be inferred from the sources and destinations of the edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices = keys.map(lambda x: (x, )).toDF([\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge = rdd.map(lambda x: (x[0], x[1])).toDF([\"src\", \"dst\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Given the Google web graph dataset, please output the list of web pages with the number of outlinks, sorted in descending order of the out-degrees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indegree and outdegree\n",
    "\n",
    "For a vertex, the number of head ends adjacent to a vertex is called the indegree of the vertex and the number of tail ends adjacent to a vertex is its outdegree (called \"branching factor\" in trees).\n",
    "\n",
    "Let G = (V, A) and v∈V. The indegree of v is denoted deg−(v) and its outdegree is denoted deg+(v).\n",
    "\n",
    "A vertex with deg−(v) = 0 is called a source, as it is the origin of each of its outcoming arrows. Similarly, a vertex with deg+(v) = 0 is called a sink, since it is the end of each of its incoming arrows.\n",
    "\n",
    "If a vertex is neither a source nor a sink, it is called an internal.[citation needed]\n",
    "If for every vertex v∈V, deg+(v) = deg−(v), the graph is called a balanced directed graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|NodeId|outDegree|\n",
      "+------+---------+\n",
      "|506742|      456|\n",
      "|203748|      372|\n",
      "|305229|      372|\n",
      "|768091|      330|\n",
      "|808643|      277|\n",
      "|412410|      268|\n",
      "|600479|      265|\n",
      "|376428|      258|\n",
      "|156950|      257|\n",
      "|885728|      256|\n",
      "|667584|      253|\n",
      "|685695|      248|\n",
      "|282140|      247|\n",
      "|598188|      245|\n",
      "|579314|      244|\n",
      "|411593|      231|\n",
      "|321091|      229|\n",
      "|838278|      225|\n",
      "|302733|      216|\n",
      "|915273|      213|\n",
      "+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g = GraphFrame(vertices, edge)\n",
    "g.outDegrees.withColumnRenamed(\"id\",\"NodeId\").sort(desc(\"outDegree\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SaveFile to CSV\n",
    "outDegreeResult = g.outDegrees.withColumnRenamed(\"id\",\"NodeId\").sort(desc(\"outDegree\"))\n",
    "outDegreeResult.coalesce(1).write.format(\"csv\").options(header='true').save(\"/home/tri/Spark/ipnyb/output/hw#5/Q1/\"+\"outDegree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Please output the inlink distribution of the top linked web pages, sorted in descending order of the in-degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|NodeId|inDegree|\n",
      "+------+--------+\n",
      "|537039|    6326|\n",
      "|597621|    5354|\n",
      "|504140|    5271|\n",
      "|751384|    5182|\n",
      "| 32163|    5097|\n",
      "|885605|    4847|\n",
      "|163075|    4731|\n",
      "|819223|    4620|\n",
      "|605856|    4550|\n",
      "|828963|    4484|\n",
      "|551829|    4220|\n",
      "| 41909|    4219|\n",
      "|558791|    4206|\n",
      "|459074|    4187|\n",
      "|407610|    4180|\n",
      "|213432|    4084|\n",
      "|765334|    4015|\n",
      "|384666|    4010|\n",
      "|173976|    3988|\n",
      "|687325|    3956|\n",
      "+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g.inDegrees.withColumnRenamed(\"id\",\"NodeId\").sort(desc(\"inDegree\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inDegreesResult = g.inDegrees.withColumnRenamed(\"id\",\"NodeId\").sort(desc(\"inDegree\"))\n",
    "inDegreesResult.coalesce(1).write.format(\"csv\").options(header='true').save(\"/home/tri/Spark/ipnyb/output/hw#5/Q2/\"+\"inDegree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Design an algorithm that maintains the connectivity of two nodes in an efficient way. Given a node v, please output the list of nodes that v points to, and the list of nodes that points to v. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Compute the PageRank of the graph using MapReduce.\n",
    "\n",
    "### PageRank\n",
    "\n",
    "There are two implementations of PageRank.\n",
    "\n",
    "- The first implementation uses the standalone GraphFrame interface and runs PageRank for a fixed number of iterations. This can be run by setting maxIter.\n",
    "\n",
    "- The second implementation uses the org.apache.spark.graphx.Pregel interface and runs PageRank until convergence. This can be run by setting tol.\n",
    "\n",
    "Both implementations support non-personalized and personalized PageRank, where setting a sourceId personalizes the results for that vertex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+\n",
      "|    id|         pagerank|\n",
      "+------+-----------------+\n",
      "| 41909|812.1452403095795|\n",
      "|597621|811.9656164662221|\n",
      "|163075|799.5590536805101|\n",
      "|537039|790.7848548128268|\n",
      "|384666|691.5460438480312|\n",
      "|504140| 673.807472416884|\n",
      "|486980|653.0737401672387|\n",
      "|605856|639.4883718897789|\n",
      "|558791| 634.690891666088|\n",
      "| 32163|628.9900597536641|\n",
      "|551829|626.2736300380823|\n",
      "|765334| 595.099059888169|\n",
      "|751384|588.4882984660073|\n",
      "|425770|552.1132327933444|\n",
      "|908351|544.5875812030654|\n",
      "|173976|543.1302882410182|\n",
      "|  7314|531.2650816652405|\n",
      "|213432|527.5442402285704|\n",
      "|885605|521.1035964936184|\n",
      "|691633|517.6216166344138|\n",
      "+------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Run PageRank algorithm, and show results.\n",
    "results = g.pageRank(resetProbability=0.15, maxIter=10)\n",
    "results.vertices.select(\"id\",\"pagerank\").sort(\"pagerank\",ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SaveFile to CSV\n",
    "PageRankResult = results.vertices.select(\"id\",\"pagerank\").sort(\"pagerank\",ascending=False)\n",
    "PageRankResult.coalesce(1).write.format(\"csv\").options(header='true').save(\"/home/tri/Spark/ipnyb/output/hw#5/Q4/\"+\"PageRank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References :\n",
    "1. Apache Spark GraphFrames - https://graphframes.github.io/quick-start.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
