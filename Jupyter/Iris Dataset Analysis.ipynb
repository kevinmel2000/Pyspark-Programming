{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/machine-learning-with-pyspark-and-mllib-solving-a-binary-classification-problem-96396065d2aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load pyspark's libraries\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "#from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "conf.setMaster(\"spark://sparklab1:7077\")\n",
    "conf.setAppName(\"Spark - Linear Regression Iris Dataset\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "sqlContext=SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://sparklab1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://sparklab1:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://sparklab1:7077 appName=PySparkShell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+----------+-------+\n",
      "|SepalLength|SepalWidth|PetalLength|PetalWidth|Species|\n",
      "+-----------+----------+-----------+----------+-------+\n",
      "|        5.1|       3.5|        1.4|       0.2| setosa|\n",
      "|        4.9|       3.0|        1.4|       0.2| setosa|\n",
      "|        4.7|       3.2|        1.3|       0.2| setosa|\n",
      "|        4.6|       3.1|        1.5|       0.2| setosa|\n",
      "|        5.0|       3.6|        1.4|       0.2| setosa|\n",
      "|        5.4|       3.9|        1.7|       0.4| setosa|\n",
      "|        4.6|       3.4|        1.4|       0.3| setosa|\n",
      "|        5.0|       3.4|        1.5|       0.2| setosa|\n",
      "|        4.4|       2.9|        1.4|       0.2| setosa|\n",
      "|        4.9|       3.1|        1.5|       0.1| setosa|\n",
      "|        5.4|       3.7|        1.5|       0.2| setosa|\n",
      "|        4.8|       3.4|        1.6|       0.2| setosa|\n",
      "|        4.8|       3.0|        1.4|       0.1| setosa|\n",
      "|        4.3|       3.0|        1.1|       0.1| setosa|\n",
      "|        5.8|       4.0|        1.2|       0.2| setosa|\n",
      "|        5.7|       4.4|        1.5|       0.4| setosa|\n",
      "|        5.4|       3.9|        1.3|       0.4| setosa|\n",
      "|        5.1|       3.5|        1.4|       0.3| setosa|\n",
      "|        5.7|       3.8|        1.7|       0.3| setosa|\n",
      "|        5.1|       3.8|        1.5|       0.3| setosa|\n",
      "+-----------+----------+-----------+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load dataset with CSV format\n",
    "df = sqlContext.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\",\"true\").load(\"/home/tri/Dataset/iris2.csv\")\n",
    "df.show()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert target into numerical categories\n",
    "labelIndexer = StringIndexer(inputCol=\"Species\", outputCol=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "#Split the data into train and test\n",
    "# To proceed, we will first randomly split the dataset into training set (70%) and test set (30%).\n",
    "trainData, testData = df.randomSplit([0.7, 0.3], seed = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecAssembler = VectorAssembler(inputCols=[\"SepalLength\", \"SepalWidth\", \"PetalLength\", \"PetalWidth\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binomial & Multinominal logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol='features',labelCol='label',maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "#chain LabelIndexer, vecAssembler and NBmodel in a\n",
    "pipeline = Pipeline(stages=[labelIndexer, vecAssembler, lr])\n",
    "#Run stages in pipeline and train model\n",
    "lrmodel = pipeline.fit(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show the predictions with Binomial Logistic Regression\n",
      "+-----+--------------------+--------------------+----------+\n",
      "|label|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+----------+\n",
      "|  2.0|[0.10407882020047...|[0.30858364739948...|       2.0|\n",
      "|  2.0|[0.10407882020047...|[0.26207532102119...|       2.0|\n",
      "|  2.0|[0.10407882020047...|[0.24074033492333...|       2.0|\n",
      "|  2.0|[0.10407882020047...|[0.26520024658990...|       2.0|\n",
      "|  2.0|[0.10407882020047...|[0.26928666885472...|       2.0|\n",
      "|  2.0|[0.10407882020047...|[0.27635713491639...|       2.0|\n",
      "|  2.0|[0.10407882020047...|[0.27039988799035...|       2.0|\n",
      "|  2.0|[0.10407882020047...|[0.24565579352269...|       2.0|\n",
      "|  2.0|[0.10407882020047...|[0.26302096558247...|       2.0|\n",
      "|  2.0|[0.10407882020047...|[0.26288537663731...|       2.0|\n",
      "|  2.0|[0.10407882020047...|[0.28011282195439...|       2.0|\n",
      "|  2.0|[0.10407882020047...|[0.26057055879094...|       2.0|\n",
      "|  2.0|[0.10407882020047...|[0.25390082463639...|       2.0|\n",
      "|  2.0|[0.10407882020047...|[0.25870336859094...|       2.0|\n",
      "|  2.0|[0.10407882020047...|[0.25839169319421...|       2.0|\n",
      "|  2.0|[0.10407882020047...|[0.24710796855748...|       2.0|\n",
      "|  2.0|[0.10407882020047...|[0.24710796855748...|       2.0|\n",
      "|  0.0|[0.10407882020047...|[0.38094552399962...|       1.0|\n",
      "|  2.0|[0.10407882020047...|[0.25172492703873...|       2.0|\n",
      "|  0.0|[0.10407882020047...|[0.36996728958340...|       0.0|\n",
      "+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on testData so we can measure the accuracy of our model on new data\n",
    "predictions = lrmodel.transform(testData)\n",
    "print(\"Show the predictions with Binomial Logistic Regression\")\n",
    "predictions.select(\"label\",\"rawPrediction\",\"probability\",\"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multinomial family\n",
    "mlr = LogisticRegression(featuresCol='features',labelCol='label',maxIter=10, regParam=0.3, elasticNetParam=0.8,family=\"multinomial\")\n",
    "#chain LabelIndexer, vecAssembler and NBmodel in a\n",
    "pipeline = Pipeline(stages=[labelIndexer, vecAssembler, mlr])\n",
    "#Run stages in pipeline and train model\n",
    "mlrmodel = pipeline.fit(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show the predictions with Multinomial Logistic Regression\n",
      "+-----+--------------------+--------------------+----------+\n",
      "|label|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+----------+\n",
      "|  2.0|[0.10407882020047...|[0.30858364739948...|       2.0|\n",
      "|  2.0|[0.10407882020047...|[0.26207532102119...|       2.0|\n",
      "|  2.0|[0.10407882020047...|[0.24074033492333...|       2.0|\n",
      "|  2.0|[0.10407882020047...|[0.26520024658990...|       2.0|\n",
      "|  2.0|[0.10407882020047...|[0.26928666885472...|       2.0|\n",
      "|  2.0|[0.10407882020047...|[0.27635713491639...|       2.0|\n",
      "|  2.0|[0.10407882020047...|[0.27039988799035...|       2.0|\n",
      "|  2.0|[0.10407882020047...|[0.24565579352269...|       2.0|\n",
      "|  2.0|[0.10407882020047...|[0.26302096558247...|       2.0|\n",
      "|  2.0|[0.10407882020047...|[0.26288537663731...|       2.0|\n",
      "|  2.0|[0.10407882020047...|[0.28011282195439...|       2.0|\n",
      "|  2.0|[0.10407882020047...|[0.26057055879094...|       2.0|\n",
      "|  2.0|[0.10407882020047...|[0.25390082463639...|       2.0|\n",
      "|  2.0|[0.10407882020047...|[0.25870336859094...|       2.0|\n",
      "|  2.0|[0.10407882020047...|[0.25839169319421...|       2.0|\n",
      "|  2.0|[0.10407882020047...|[0.24710796855748...|       2.0|\n",
      "|  2.0|[0.10407882020047...|[0.24710796855748...|       2.0|\n",
      "|  0.0|[0.10407882020047...|[0.38094552399962...|       1.0|\n",
      "|  2.0|[0.10407882020047...|[0.25172492703873...|       2.0|\n",
      "|  0.0|[0.10407882020047...|[0.36996728958340...|       0.0|\n",
      "+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on testData so we can measure the accuracy of our model on new data\n",
    "predictions = mlrmodel.transform(testData)\n",
    "print(\"Show the predictions with Multinomial Logistic Regression\")\n",
    "predictions.select(\"label\",\"rawPrediction\",\"probability\",\"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objectiveHistory:\n",
      "1.09594721312\n",
      "1.0919755203\n",
      "1.078547384\n",
      "1.05223211607\n",
      "1.05136797957\n",
      "1.04431994213\n",
      "1.04336430478\n",
      "1.04295122409\n",
      "1.04261194695\n",
      "1.04212565358\n",
      "1.04045031408\n"
     ]
    }
   ],
   "source": [
    "trainingSummary = lrmodel.stages[-1].summary\n",
    "# Obtain the objective per iteration\n",
    "objectiveHistory = trainingSummary.objectiveHistory\n",
    "print(\"objectiveHistory:\")\n",
    "for objective in objectiveHistory:\n",
    "    print(objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positive rate by label:\n",
      "label 0: 0.0\n",
      "label 1: 0.411764705882\n",
      "label 2: 0.0\n",
      "True positive rate by label:\n",
      "label 0: 0.243243243243\n",
      "label 1: 1.0\n",
      "label 2: 1.0\n",
      "Precision by label:\n",
      "label 0: 1.0\n",
      "label 1: 0.555555555556\n",
      "label 2: 1.0\n",
      "Recall by label:\n",
      "label 0: 0.243243243243\n",
      "label 1: 1.0\n",
      "label 2: 1.0\n",
      "F-measure by label:\n",
      "label 0: 0.391304347826\n",
      "label 1: 0.714285714286\n",
      "label 2: 1.0\n"
     ]
    }
   ],
   "source": [
    "# for multiclass, we can inspect metrics on a per-label basis\n",
    "print(\"False positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.falsePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "print(\"True positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.truePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "print(\"Precision by label:\")\n",
    "for i, prec in enumerate(trainingSummary.precisionByLabel):\n",
    "    print(\"label %d: %s\" % (i, prec))\n",
    "\n",
    "print(\"Recall by label:\")\n",
    "for i, rec in enumerate(trainingSummary.recallByLabel):\n",
    "    print(\"label %d: %s\" % (i, rec))\n",
    "\n",
    "print(\"F-measure by label:\")\n",
    "for i, f in enumerate(trainingSummary.fMeasureByLabel()):\n",
    "    print(\"label %d: %s\" % (i, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.728155339806\n",
      "FPR: 0.139920045688\n",
      "TPR: 0.728155339806\n",
      "F-measure: 0.684254959899\n",
      "Precision: 0.848975188781\n",
      "Recall: 0.728155339806\n"
     ]
    }
   ],
   "source": [
    "accuracy = trainingSummary.accuracy\n",
    "falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
    "truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
    "fMeasure = trainingSummary.weightedFMeasure()\n",
    "precision = trainingSummary.weightedPrecision\n",
    "recall = trainingSummary.weightedRecall\n",
    "print(\"Accuracy: %s\\nFPR: %s\\nTPR: %s\\nF-measure: %s\\nPrecision: %s\\nRecall: %s\"\n",
    "      % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.744680851064\n",
      "Precision: 0.858156028369\n",
      "Recall: 0.744680851064\n",
      "f1: 0.671732522796\n"
     ]
    }
   ],
   "source": [
    "evaluator_accuracy_lr = MulticlassClassificationEvaluator(labelCol=\"label\", \n",
    "                                                          predictionCol=\"prediction\",metricName=\"accuracy\")\n",
    "accuracy_lr = evaluator_accuracy_lr.evaluate(predictions)\n",
    "evaluator_precision_lr = MulticlassClassificationEvaluator(labelCol=\"label\", \n",
    "                                                           predictionCol=\"prediction\",metricName=\"weightedPrecision\")\n",
    "wPrecision_lr = evaluator_precision_lr.evaluate(predictions)\n",
    "evaluator_recall_lr = MulticlassClassificationEvaluator(labelCol=\"label\", \n",
    "                                                        predictionCol=\"prediction\",metricName=\"weightedRecall\")\n",
    "wRecall_lr = evaluator_recall_lr.evaluate(predictions)\n",
    "evaluator_fone_lr = MulticlassClassificationEvaluator(labelCol=\"label\", \n",
    "                                                      predictionCol=\"prediction\",metricName=\"f1\")\n",
    "fone_lr = evaluator_fone_lr.evaluate(predictions)\n",
    "print(\"Accuracy: %s\\nPrecision: %s\\nRecall: %s\\nf1: %s\" \n",
    "      %(accuracy_lr, wPrecision_lr, wRecall_lr, fone_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Test Area Under ROC', 0.4230769230769231)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print('Test Area Under ROC', evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "====================================\n",
      "Root Mean Squared Error (RMSE)= 0.505291\n",
      "r2 on test data = 0.615804\n",
      "Mean squared error on test data = 0.255319\n",
      "Mean absolute error on test data = 0.255319\n"
     ]
    }
   ],
   "source": [
    "evaluator_rmse = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator_rmse.evaluate(predictions)\n",
    "evaluator_r2 = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2 = evaluator_r2.evaluate(predictions)\n",
    "evaluator_mse = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mse\")\n",
    "mse = evaluator_mse.evaluate(predictions)\n",
    "evaluator_mae = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "mae = evaluator_mae.evaluate(predictions)\n",
    "print(\"Logistic Regression\")\n",
    "print(\"====================================\")\n",
    "print(\"Root Mean Squared Error (RMSE)= %g\" % rmse)\n",
    "print(\"r2 on test data = %g\" % r2)\n",
    "print(\"Mean squared error on test data = %g\" % mse)\n",
    "print(\"Mean absolute error on test data = %g\" % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train a NaiveBayes Model\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "\n",
    "#chain LabelIndexer, vecAssembler and NBmodel in a\n",
    "pipeline = Pipeline(stages=[labelIndexer, vecAssembler, nb])\n",
    "\n",
    "#Run stages in pipeline and train model\n",
    "nbmodel = pipeline.fit(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show the predictions\n",
      "+-----+--------------------+--------------------+----------+\n",
      "|label|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+----------+\n",
      "|  2.0|[-11.047968119928...|[0.29698237383357...|       2.0|\n",
      "|  2.0|[-13.061767677519...|[0.21798511589122...|       2.0|\n",
      "|  2.0|[-12.669120847178...|[0.15141773405828...|       2.0|\n",
      "|  2.0|[-12.463850673847...|[0.20436085847774...|       2.0|\n",
      "|  2.0|[-13.605849867833...|[0.24788699074567...|       2.0|\n",
      "|  2.0|[-12.432913431454...|[0.22533897917645...|       2.0|\n",
      "|  2.0|[-12.480992262406...|[0.21072257132948...|       2.0|\n",
      "|  2.0|[-13.180362745086...|[0.16530848479213...|       2.0|\n",
      "|  2.0|[-12.607246362392...|[0.18841991732741...|       2.0|\n",
      "|  2.0|[-13.013228681266...|[0.20023264276145...|       2.0|\n",
      "|  2.0|[-14.175057135508...|[0.28149718404605...|       2.0|\n",
      "|  2.0|[-13.386370707735...|[0.20037174080684...|       2.0|\n",
      "|  2.0|[-13.429478177966...|[0.18351954523753...|       2.0|\n",
      "|  2.0|[-15.000357261790...|[0.24433103432406...|       2.0|\n",
      "|  2.0|[-13.353530415329...|[0.18864884421606...|       2.0|\n",
      "|  2.0|[-14.054803948022...|[0.17451529522505...|       2.0|\n",
      "|  2.0|[-14.142921938497...|[0.17250580082906...|       2.0|\n",
      "|  0.0|[-17.564473712879...|[0.49917720827077...|       0.0|\n",
      "|  2.0|[-13.660991856986...|[0.16640704731458...|       2.0|\n",
      "|  0.0|[-17.385874516615...|[0.50561466273043...|       0.0|\n",
      "+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on testData so we can measure the accuracy of our model on new data\n",
    "predictions = nbmodel.transform(testData)\n",
    "print(\"Show the predictions\")\n",
    "predictions.select(\"label\",\"rawPrediction\",\"probability\",\"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Test Area Under ROC', 0.5791855203619909)\n"
     ]
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator()\n",
    "print('Test Area Under ROC', evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes\n",
      "====================================\n",
      "Root Mean Squared Error (RMSE)= 0.252646\n",
      "r2 on test data = 0.903951\n",
      "Mean squared error on test data = 0.0638298\n",
      "Mean absolute error on test data = 0.0638298\n"
     ]
    }
   ],
   "source": [
    "evaluator_rmse = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator_rmse.evaluate(predictions)\n",
    "evaluator_r2 = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2 = evaluator_r2.evaluate(predictions)\n",
    "evaluator_mse = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mse\")\n",
    "mse = evaluator_mse.evaluate(predictions)\n",
    "evaluator_mae = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "mae = evaluator_mae.evaluate(predictions)\n",
    "print(\"Naive Bayes\")\n",
    "print(\"====================================\")\n",
    "print(\"Root Mean Squared Error (RMSE)= %g\" % rmse)\n",
    "print(\"r2 on test data = %g\" % r2)\n",
    "print(\"Mean squared error on test data = %g\" % mse)\n",
    "print(\"Mean absolute error on test data = %g\" % mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_accuracy_nb = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                                          metricName=\"accuracy\")\n",
    "accuracy_nb = evaluator_accuracy_nb.evaluate(predictions)\n",
    "evaluator_precision_nb = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                                           metricName=\"weightedPrecision\")\n",
    "wPrecision_nb = evaluator_precision_nb.evaluate(predictions)\n",
    "evaluator_recall_nb = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                                     metricName=\"weightedRecall\")\n",
    "wRecall_nb = evaluator_recall_nb.evaluate(predictions)\n",
    "evaluator_fone_nb = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"f1\")\n",
    "fone_nb = evaluator_fone_nb.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================\n",
      "Naive Bayes\n",
      "====================================\n",
      "Test set accuracy = 0.936170212766\n",
      "Test set Precision = 0.948138297872\n",
      "Test set Recall = 0.936170212766\n",
      "Test set f1 = 0.935925654194\n"
     ]
    }
   ],
   "source": [
    "print(\"====================================\")\n",
    "print(\"Naive Bayes\")\n",
    "print(\"====================================\")\n",
    "print(\"Test set accuracy = \" + str(accuracy_nb))\n",
    "print(\"Test set Precision = \" + str(wPrecision_nb))\n",
    "print(\"Test set Recall = \" + str(wRecall_nb))\n",
    "print(\"Test set f1 = \" + str(fone_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show the predictions\n",
      "+-----+--------------+-------------+----------+\n",
      "|label| rawPrediction|  probability|prediction|\n",
      "+-----+--------------+-------------+----------+\n",
      "|  2.0|[0.0,0.0,31.0]|[0.0,0.0,1.0]|       2.0|\n",
      "|  2.0|[0.0,0.0,31.0]|[0.0,0.0,1.0]|       2.0|\n",
      "|  2.0|[0.0,0.0,31.0]|[0.0,0.0,1.0]|       2.0|\n",
      "|  2.0|[0.0,0.0,31.0]|[0.0,0.0,1.0]|       2.0|\n",
      "|  2.0|[0.0,0.0,31.0]|[0.0,0.0,1.0]|       2.0|\n",
      "|  2.0|[0.0,0.0,31.0]|[0.0,0.0,1.0]|       2.0|\n",
      "|  2.0|[0.0,0.0,31.0]|[0.0,0.0,1.0]|       2.0|\n",
      "|  2.0|[0.0,0.0,31.0]|[0.0,0.0,1.0]|       2.0|\n",
      "|  2.0|[0.0,0.0,31.0]|[0.0,0.0,1.0]|       2.0|\n",
      "|  2.0|[0.0,0.0,31.0]|[0.0,0.0,1.0]|       2.0|\n",
      "|  2.0|[0.0,0.0,31.0]|[0.0,0.0,1.0]|       2.0|\n",
      "|  2.0|[0.0,0.0,31.0]|[0.0,0.0,1.0]|       2.0|\n",
      "|  2.0|[0.0,0.0,31.0]|[0.0,0.0,1.0]|       2.0|\n",
      "|  2.0|[0.0,0.0,31.0]|[0.0,0.0,1.0]|       2.0|\n",
      "|  2.0|[0.0,0.0,31.0]|[0.0,0.0,1.0]|       2.0|\n",
      "|  2.0|[0.0,0.0,31.0]|[0.0,0.0,1.0]|       2.0|\n",
      "|  2.0|[0.0,0.0,31.0]|[0.0,0.0,1.0]|       2.0|\n",
      "|  0.0|[34.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|  2.0|[0.0,0.0,31.0]|[0.0,0.0,1.0]|       2.0|\n",
      "|  0.0|[34.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "+-----+--------------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(featuresCol = 'features',labelCol='label')\n",
    "pipeline = Pipeline(stages=[labelIndexer, vecAssembler, dt])\n",
    "dtModel = pipeline.fit(trainData)\n",
    "\n",
    "predictions = dtModel.transform(testData)\n",
    "print(\"Show the predictions\")\n",
    "predictions.select(\"label\",\"rawPrediction\",\"probability\",\"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.145865\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VectorAssembler_45efabff5cb927255827\n"
     ]
    }
   ],
   "source": [
    "treeModel = dtModel.stages[1]\n",
    "# summary only\n",
    "print(treeModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----------------+\n",
      "|prediction|label|         features|\n",
      "+----------+-----+-----------------+\n",
      "|       1.8|  2.0|[4.5,2.3,1.3,0.3]|\n",
      "|       2.0|  2.0|[4.6,3.4,1.4,0.3]|\n",
      "|       2.0|  2.0|[4.6,3.6,1.0,0.2]|\n",
      "|       2.0|  2.0|[4.7,3.2,1.3,0.2]|\n",
      "|       2.0|  2.0|[4.8,3.4,1.9,0.2]|\n",
      "|       2.0|  2.0|[4.9,3.0,1.4,0.2]|\n",
      "|       2.0|  2.0|[4.9,3.1,1.5,0.1]|\n",
      "|       2.0|  2.0|[4.9,3.6,1.4,0.1]|\n",
      "|       2.0|  2.0|[5.0,3.2,1.2,0.2]|\n",
      "|       2.0|  2.0|[5.0,3.3,1.4,0.2]|\n",
      "|       2.0|  2.0|[5.1,3.3,1.7,0.5]|\n",
      "|       2.0|  2.0|[5.1,3.4,1.5,0.2]|\n",
      "|       2.0|  2.0|[5.1,3.5,1.4,0.2]|\n",
      "|       2.0|  2.0|[5.1,3.8,1.9,0.4]|\n",
      "|       2.0|  2.0|[5.2,3.4,1.4,0.2]|\n",
      "|       2.0|  2.0|[5.3,3.7,1.5,0.2]|\n",
      "|       2.0|  2.0|[5.4,3.7,1.5,0.2]|\n",
      "|       0.0|  0.0|[5.5,2.3,4.0,1.3]|\n",
      "|       2.0|  2.0|[5.5,3.5,1.3,0.2]|\n",
      "|       0.0|  0.0|[5.6,2.5,3.9,1.1]|\n",
      "+----------+-----+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(featuresCol='features',labelCol= 'label', numTrees=10)\n",
    "\n",
    "pipeline = Pipeline(stages=[labelIndexer, vecAssembler, rf])\n",
    "rfModel = pipeline.fit(trainData)\n",
    "predictions = rfModel.transform(testData)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.161768\n",
      "Test Error = 0.191489\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
