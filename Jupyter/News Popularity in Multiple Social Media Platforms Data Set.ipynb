{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Mining: HW#2 \n",
    "# News Popularity in Multiple Social Media Platforms Data Set\n",
    "\n",
    "### Members :\n",
    "1. <b>Tri Wanda Septian (106998406)\n",
    "2. <b>Anggara Aji Saputra (106998412)\n",
    "\n",
    "### Input data\n",
    "### Data: \n",
    "- [News Popularity in Multiple Social Media Platforms dataset] from UCI Machine Learning Repository\n",
    "- About 100,000 news items on four different topics: economy, microsoft, Obama, and Palestine, during November 2015 and July 2016\n",
    "- Available at<br>\n",
    "https://archive.ics.uci.edu/ml/datasets/News+Popularity+in+Multiple+Social+Media+Platforms \n",
    "\n",
    "### Format:\n",
    "- News data: 1 CSV file\n",
    "- Social feedback data: CSV files for 4 topics on 3 platforms\n",
    "\n",
    "### Attributes of News Data\n",
    "- IDLink (numeric): Unique identifier of news items \n",
    "- Title (string): Title of the news item according to the official media sources \n",
    "- Headline (string): Headline of the news item according to the official media sources \n",
    "- Source (string): Original news outlet that published the news item \n",
    "- Topic (string): Query topic used to obtain the items in the official media sources \n",
    "- PublishDate (timestamp): Date and time of the news items' publication \n",
    "- SentimentTitle (numeric): Sentiment score of the text in the news items' title \n",
    "- SentimentHeadline (numeric): Sentiment score of the text in the news items' headline \n",
    "- Facebook (numeric): Final value of the news items' popularity according to the social media source Facebook \n",
    "- GooglePlus (numeric): Final value of the news items' popularity according to the social media source Google+ \n",
    "- LinkedIn (numeric): Final value of the news items' popularity according to the social media source LinkedIn \n",
    "\n",
    "### Attributes of Social Feedback Data\n",
    "- IDLink (numeric): Unique identifier of news items \n",
    "- TS1 (numeric): Level of popularity in time slice 1 (0-20 minutes upon publication) \n",
    "- TS2 (numeric): Level of popularity in time slice 2 (20-40 minutes upon publication) \n",
    "- TS... (numeric): Level of popularity in time slice ... \n",
    "- TS144 (numeric): Final level of popularity after 2 days upon publication\n",
    "\n",
    "### Task Description\n",
    "### Subtasks\n",
    "- (30pt) (1) In news data, count the words in two fields: ‘Title’ and ‘Headline’ respectively, and list the most frequent words according to the term frequency in descending order, in total, per day, and per topic, respectively\n",
    "- (20pt) (2) In social feedback data, calculate the average popularity of each news by hour, and by day, respectively (for each platform)\n",
    "- (20pt) (3) In news data, calculate the sum and average sentiment score of each topic, respectively\n",
    "- (30pt) (4) From subtask (1), for the top-100 frequent words per topic in titles and headlines, calculate their co-occurrence matrices (100x100), respectively. Each entry in the matrix will contain the co-occurrence frequency in all news titles and headlines, respectively\n",
    "\n",
    "### Output Format\n",
    "- (1) 6 sorted lists of top-frequent words: {in total, per day, per topic}{for titles, headlines}\n",
    "    - Each line: <word> <count>\n",
    "    - For sorted lists per day/per topic, you can also separate them into individual lists by day/by topic \n",
    "        - That will make more numbers of sorted lists: 60*2 lists by day, and 4*2 by topic. \n",
    "- (2) 6 files: {by hour, by day} {3 platforms}\n",
    "    - Each line: <avg popularity>\n",
    "- (3) 8 values: {sum, avg} {4 topics}\n",
    "- (4) 8 100x100 matrices: {title, headline} {4 topics}\n",
    "    - Each entry mij in the matrix: the co-occurrence frequency of wi and wj\n",
    "    \n",
    "### Specification and Setup\n",
    "### Cluster environment setup\n",
    "We are using a Virtual Machine (1 unit) and in it running docker services (2 units). All it is already running in Server.\n",
    "\n",
    "VM’s specification (Master Computer) :\n",
    "- Operating System \t: Ubuntu 16.04.4 LTS “Desktop Version”\n",
    "- MemTotal\t\t: 8168740 kB\n",
    "- MemFree\t\t: 488156 kB\n",
    "- MemAvailable\t\t: 2062380 kB\n",
    "- Buffers\t\t\t: 126724 kB\n",
    "- Docker’s images specification (Slave 1, Slave 2):\n",
    "- Operating System\t: Ubuntu 16.04.4 LTS \n",
    "\n",
    "\n",
    "Network Topology Setup\n",
    "<br>Figure.1 shows the master is running two dockers services for hadoop cluster nodes and master computer has ip gateway (172.17.0.1) is connecting to each hadoop cluster nodes (Slave 1: 172.17.0.2;Slave 2: 172.17.17.3) . For Spark services master computer is running to the two workers in single node.\n",
    "![title](img/topology.png)\n",
    "\n",
    "### Hadoop master and slaves environtment process\n",
    "\n",
    "![title](img/1.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory (RAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MemTotal:       12039640 kB\r\n",
      "MemFree:         3880964 kB\r\n",
      "MemAvailable:    5296064 kB\r\n",
      "Buffers:          135328 kB\r\n",
      "Cached:          1744304 kB\r\n",
      "SwapCached:        36344 kB\r\n",
      "Active:          5403468 kB\r\n",
      "Inactive:        1916436 kB\r\n",
      "Active(anon):    4593064 kB\r\n",
      "Inactive(anon):  1512236 kB\r\n",
      "Active(file):     810404 kB\r\n",
      "Inactive(file):   404200 kB\r\n",
      "Unevictable:         584 kB\r\n",
      "Mlocked:             584 kB\r\n",
      "SwapTotal:       6837244 kB\r\n",
      "SwapFree:        5768064 kB\r\n",
      "Dirty:              2444 kB\r\n",
      "Writeback:             0 kB\r\n",
      "AnonPages:       5430556 kB\r\n",
      "Mapped:           754136 kB\r\n",
      "Shmem:            665028 kB\r\n",
      "Slab:             595468 kB\r\n",
      "SReclaimable:     523652 kB\r\n",
      "SUnreclaim:        71816 kB\r\n",
      "KernelStack:       23856 kB\r\n",
      "PageTables:        87716 kB\r\n",
      "NFS_Unstable:          0 kB\r\n",
      "Bounce:                0 kB\r\n",
      "WritebackTmp:          0 kB\r\n",
      "CommitLimit:    12857064 kB\r\n",
      "Committed_AS:   21392360 kB\r\n",
      "VmallocTotal:   34359738367 kB\r\n",
      "VmallocUsed:           0 kB\r\n",
      "VmallocChunk:          0 kB\r\n",
      "HardwareCorrupted:     0 kB\r\n",
      "AnonHugePages:      2048 kB\r\n",
      "CmaTotal:              0 kB\r\n",
      "CmaFree:               0 kB\r\n",
      "HugePages_Total:       0\r\n",
      "HugePages_Free:        0\r\n",
      "HugePages_Rsvd:        0\r\n",
      "HugePages_Surp:        0\r\n",
      "Hugepagesize:       2048 kB\r\n",
      "DirectMap4k:      377472 kB\r\n",
      "DirectMap2M:    10891264 kB\r\n",
      "DirectMap1G:     2097152 kB\r\n"
     ]
    }
   ],
   "source": [
    "%cat /proc/meminfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processor\t: 0\r\n",
      "vendor_id\t: GenuineIntel\r\n",
      "cpu family\t: 6\r\n",
      "model\t\t: 69\r\n",
      "model name\t: Intel(R) Core(TM) i5-4300U CPU @ 1.90GHz\r\n",
      "stepping\t: 1\r\n",
      "microcode\t: 0x23\r\n",
      "cpu MHz\t\t: 2600.000\r\n",
      "cache size\t: 3072 KB\r\n",
      "physical id\t: 0\r\n",
      "siblings\t: 4\r\n",
      "core id\t\t: 0\r\n",
      "cpu cores\t: 2\r\n",
      "apicid\t\t: 0\r\n",
      "initial apicid\t: 0\r\n",
      "fpu\t\t: yes\r\n",
      "fpu_exception\t: yes\r\n",
      "cpuid level\t: 13\r\n",
      "wp\t\t: yes\r\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single spec_ctrl retpoline kaiser tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt dtherm ida arat pln pts\r\n",
      "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2\r\n",
      "bogomips\t: 4988.64\r\n",
      "clflush size\t: 64\r\n",
      "cache_alignment\t: 64\r\n",
      "address sizes\t: 39 bits physical, 48 bits virtual\r\n",
      "power management:\r\n",
      "\r\n",
      "processor\t: 1\r\n",
      "vendor_id\t: GenuineIntel\r\n",
      "cpu family\t: 6\r\n",
      "model\t\t: 69\r\n",
      "model name\t: Intel(R) Core(TM) i5-4300U CPU @ 1.90GHz\r\n",
      "stepping\t: 1\r\n",
      "microcode\t: 0x23\r\n",
      "cpu MHz\t\t: 2600.000\r\n",
      "cache size\t: 3072 KB\r\n",
      "physical id\t: 0\r\n",
      "siblings\t: 4\r\n",
      "core id\t\t: 0\r\n",
      "cpu cores\t: 2\r\n",
      "apicid\t\t: 1\r\n",
      "initial apicid\t: 1\r\n",
      "fpu\t\t: yes\r\n",
      "fpu_exception\t: yes\r\n",
      "cpuid level\t: 13\r\n",
      "wp\t\t: yes\r\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single spec_ctrl retpoline kaiser tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt dtherm ida arat pln pts\r\n",
      "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2\r\n",
      "bogomips\t: 4988.64\r\n",
      "clflush size\t: 64\r\n",
      "cache_alignment\t: 64\r\n",
      "address sizes\t: 39 bits physical, 48 bits virtual\r\n",
      "power management:\r\n",
      "\r\n",
      "processor\t: 2\r\n",
      "vendor_id\t: GenuineIntel\r\n",
      "cpu family\t: 6\r\n",
      "model\t\t: 69\r\n",
      "model name\t: Intel(R) Core(TM) i5-4300U CPU @ 1.90GHz\r\n",
      "stepping\t: 1\r\n",
      "microcode\t: 0x23\r\n",
      "cpu MHz\t\t: 2600.000\r\n",
      "cache size\t: 3072 KB\r\n",
      "physical id\t: 0\r\n",
      "siblings\t: 4\r\n",
      "core id\t\t: 1\r\n",
      "cpu cores\t: 2\r\n",
      "apicid\t\t: 2\r\n",
      "initial apicid\t: 2\r\n",
      "fpu\t\t: yes\r\n",
      "fpu_exception\t: yes\r\n",
      "cpuid level\t: 13\r\n",
      "wp\t\t: yes\r\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single spec_ctrl retpoline kaiser tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt dtherm ida arat pln pts\r\n",
      "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2\r\n",
      "bogomips\t: 4988.64\r\n",
      "clflush size\t: 64\r\n",
      "cache_alignment\t: 64\r\n",
      "address sizes\t: 39 bits physical, 48 bits virtual\r\n",
      "power management:\r\n",
      "\r\n",
      "processor\t: 3\r\n",
      "vendor_id\t: GenuineIntel\r\n",
      "cpu family\t: 6\r\n",
      "model\t\t: 69\r\n",
      "model name\t: Intel(R) Core(TM) i5-4300U CPU @ 1.90GHz\r\n",
      "stepping\t: 1\r\n",
      "microcode\t: 0x23\r\n",
      "cpu MHz\t\t: 2599.902\r\n",
      "cache size\t: 3072 KB\r\n",
      "physical id\t: 0\r\n",
      "siblings\t: 4\r\n",
      "core id\t\t: 1\r\n",
      "cpu cores\t: 2\r\n",
      "apicid\t\t: 3\r\n",
      "initial apicid\t: 3\r\n",
      "fpu\t\t: yes\r\n",
      "fpu_exception\t: yes\r\n",
      "cpuid level\t: 13\r\n",
      "wp\t\t: yes\r\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single spec_ctrl retpoline kaiser tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt dtherm ida arat pln pts\r\n",
      "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2\r\n",
      "bogomips\t: 4988.64\r\n",
      "clflush size\t: 64\r\n",
      "cache_alignment\t: 64\r\n",
      "address sizes\t: 39 bits physical, 48 bits virtual\r\n",
      "power management:\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "%cat /proc/cpuinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operating System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISTRIB_ID=Ubuntu\r\n",
      "DISTRIB_RELEASE=16.04\r\n",
      "DISTRIB_CODENAME=xenial\r\n",
      "DISTRIB_DESCRIPTION=\"Ubuntu 16.04.4 LTS\"\r\n"
     ]
    }
   ],
   "source": [
    "%cat /etc/lsb-release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import TimestampType, DateType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://sparklab-master:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://sparklab-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f73a1264a90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"HW#2\").getOrCreate()\n",
    "#testing spark session environtment\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news = spark.read.csv(\"/home/twster/Spark/Projects/datasets/hw2/News_Final.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- IDLink: decimal(10,0) (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- Headline: string (nullable = true)\n",
      " |-- Source: string (nullable = true)\n",
      " |-- Topic: string (nullable = true)\n",
      " |-- PublishDate: string (nullable = true)\n",
      " |-- SentimentTitle: string (nullable = true)\n",
      " |-- SentimentHeadline: string (nullable = true)\n",
      " |-- Facebook: string (nullable = true)\n",
      " |-- GooglePlus: string (nullable = true)\n",
      " |-- LinkedIn: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_news.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('IDLink', 'decimal(10,0)'),\n",
       " ('Title', 'string'),\n",
       " ('Headline', 'string'),\n",
       " ('Source', 'string'),\n",
       " ('Topic', 'string'),\n",
       " ('PublishDate', 'string'),\n",
       " ('SentimentTitle', 'string'),\n",
       " ('SentimentHeadline', 'string'),\n",
       " ('Facebook', 'string'),\n",
       " ('GooglePlus', 'string'),\n",
       " ('LinkedIn', 'string')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show types data \n",
    "df_news.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PublishDate: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#change types of PublishDate to TimestampType\n",
    "df2_news = df_news.withColumn(\"PublishDate\", unix_timestamp(\"PublishDate\", \"yyyy-MM-dd HH:mm:ss\").cast(TimestampType()))\n",
    "#check again the Schema of Dataset\n",
    "df2_news.select(\"PublishDate\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new column from splited PublishDate -> Date and Time\n",
    "split_PublishDate = F.split(df2_news['PublishDate'], ' ')\n",
    "df3_news = df2_news.withColumn('Date', split_PublishDate.getItem(0)).withColumn('Time', split_PublishDate.getItem(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change Date column to DateType\n",
    "df4_news = df3_news.withColumn('Date', df3_news['Date'].cast(DateType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show the Schema of PublisDate, Date and Time\n",
      "root\n",
      " |-- PublishDate: timestamp (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#show the new dataframe\n",
    "print \"Show the Schema of PublisDate, Date and Time\"\n",
    "df4_news.select(\"PublishDate\",\"Date\",\"Time\").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) In news data, count the words in two fields: ‘Title’ and ‘Headline’ respectively, and list the most frequent words according to the term frequency in descending order, in total, per day, and per topic, respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TitleWords = df4_news.withColumn(\"Title\", explode(split(\"Title\", \" \")))\n",
    "\n",
    "TitleWords.groupBy(\"Title\").count().orderBy(desc(\"count\"))\\\n",
    "            .withColumnRenamed(\"Title\",\"Words in Title\")\\\n",
    "            .coalesce(1).write.format(\"csv\").options (header='true')\\\n",
    "            .save(\"output/Q1/\"+\"Wordcount_Title_in_Total\")\n",
    "            \n",
    "TitleWords.groupBy(\"Title\",\"Topic\").count().orderBy(desc(\"count\"))\\\n",
    "            .withColumnRenamed(\"Title\",\"Words in Title\")\\\n",
    "            .coalesce(1).write.format(\"csv\").options (header='true')\\\n",
    "            .save(\"output/Q1/\"+\"Wordcount_Title_per_Topic\")\n",
    "            \n",
    "TitleWords.groupBy(\"Title\",\"PublishDate\").count()\\\n",
    "            .withColumnRenamed(\"Title\",\"Word in Title\")\\\n",
    "            .coalesce(1).write.format(\"csv\").options (header='true')\\\n",
    "            .save(\"output/Q1/\"+\"Wordcount_Title_PublishDate\")\n",
    "            \n",
    "TitleWords.groupBy(\"Title\",\"Date\").count().withColumnRenamed(\"Title\",\"Words in Title\")\\\n",
    "            .coalesce(1).write.format(\"csv\").options(header='true')\\\n",
    "            .save(\"output/Q1/\"+\"Wordcount_Title_per_Day\")\n",
    "        \n",
    "TitleWords.groupBy(\"Title\",\"Time\").count().withColumnRenamed(\"Title\",\"Words in Title\")\\\n",
    "        .coalesce(1).write.format(\"csv\").options(header='true')\\\n",
    "        .save(\"output/Q1/\"+\"Wordcount_Title_per_Time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "HeadlineWords = df4_news.withColumn(\"Headline\", explode(split(\"Headline\", \" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "HeadlineWords.groupBy(\"Headline\").count().orderBy(desc(\"count\"))\\\n",
    "                .coalesce(1).write.format(\"csv\").options(header='true')\\\n",
    "                .save(\"output/Q1/\"+\"Wordcount_Headline\")\n",
    "\n",
    "HeadlineWords.groupBy(\"Headline\",\"Topic\").count().orderBy(desc(\"count\"))\\\n",
    "                .withColumnRenamed(\"Headline\",\"Words in Headline\")\\\n",
    "                .coalesce(1).write.format(\"csv\").options(header='true')\\\n",
    "                .save(\"output/Q1/\"+\"Wordcount_Headline_per_Topic\")\n",
    "\n",
    "HeadlineWords.groupBy(\"Headline\",\"PublishDate\").count()\\\n",
    "                .coalesce(1).write.format(\"csv\").options(header='true')\\\n",
    "                .save(\"output/Q1/\"+\"Wordcount_Headline_per_PublishDate\")\n",
    "\n",
    "HeadlineWords.groupBy(\"Headline\",\"Date\").count()\\\n",
    "                .coalesce(1).write.format(\"csv\").options(header='true')\\\n",
    "                .save(\"output/Q1/\"+\"Wordcount_Headline_per_Date\")\n",
    "        \n",
    "HeadlineWords.groupBy(\"Headline\",\"Time\").count()\\\n",
    "                .coalesce(1).write.format(\"csv\").options(header='true')\\\n",
    "                .save(\"output/Q1/\"+\"Wordcount_headline_per_Time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) In social feedback data, calculate the average popularity of each news by hour, and by day, respectively (for each platform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in this case,we are using rdd\n",
    "#list files from dataset\n",
    "fileFBList=['Facebook_Economy', 'Facebook_Microsoft', 'Facebook_Obama', 'Facebook_Palestine']\n",
    "fileGPlusList=['GooglePlus_Economy', 'GooglePlus_Microsoft', 'GooglePlus_Obama', 'GooglePlus_Palestine']\n",
    "fileLinkedInList=['LinkedIn_Economy', 'LinkedIn_Microsoft', 'LinkedIn_Obama', 'LinkedIn_Palestine']\n",
    "\n",
    "header_per_hour=['IDLink'] + ['TS'+str((count+1)*3) for count in range(48)]\n",
    "header_per_day=['IDLink'] + ['TS'+str((count+1)*72) for count in range(2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feedback From Facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "#Social media Facebook\n",
    "SocMedia = 'Facebook'\n",
    "\n",
    "Facebook_per_Hour = open('output/Q2/'+'Facebook_per_Hour.csv','w')\n",
    "Facebook_per_Day = open('output/Q2/'+'Facebook_per_Day.csv','w')\n",
    "\n",
    "for FileFB in fileFBList:\n",
    "    facebook_feedback = spark.read.csv('/home/twster/Spark/Projects/datasets/hw2/'+FileFB+'.csv'\n",
    "                                       ,header=True,inferSchema=True)\n",
    "    popularity_avg_perhour = facebook_feedback.select(header_per_hour)\\\n",
    "                            .rdd.map(list).flatMap(lambda x:((x[0], element) for element in x[1:]))\\\n",
    "                            .reduceByKey(add).map(lambda x:(x[0], x[1]/48)).sortByKey()\\\n",
    "                            .map(lambda x:('ID'+str(x[0]), x[1])).collect()\n",
    "    popularity_avg_perday = facebook_feedback.select(header_per_day)\\\n",
    "                            .rdd.map(list).flatMap(lambda x:((x[0], element) for element in x[1:]))\\\n",
    "                            .reduceByKey(add).map(lambda x:(x[0], x[1]/2)).sortByKey()\\\n",
    "                            .map(lambda x:('ID'+str(x[0]), x[1])).collect()\n",
    "    \n",
    "    if FileFB.split('_')[0] == SocMedia:\n",
    "        Facebook_per_hour = open('output/Q2/'+'Facebook_per_Hour.csv','w')\n",
    "        Facebook_per_day = open('output/Q2/'+'Facebook_per_Day.csv','w')\n",
    "        for j in popularity_avg_perhour:\n",
    "            k = ':'.join([str(l)for l in j])\n",
    "            Facebook_per_hour.write(k + '\\n')\n",
    "        for j in popularity_avg_perday:\n",
    "            k = ':'.join([str(l)for l in j])\n",
    "            Facebook_per_day.write(k + '\\n')\n",
    "        Facebook_per_hour.close()\n",
    "        Facebook_per_day.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feedback From GooglePlus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Social media GooglePlus\n",
    "SocMedia = 'GooglePlus'\n",
    "\n",
    "GooglePlus_per_Hour = open('output/Q2/'+'GooglePlus_per_Hour.csv','w')\n",
    "GooglePlus_per_Day = open('output/Q2/'+'GooglePlus_per_Day.csv','w')\n",
    "\n",
    "for FileGPlus in fileGPlusList:\n",
    "    gplus_feedback = spark.read.csv('/home/twster/Spark/Projects/datasets/hw2/'+FileGPlus+'.csv'\n",
    "                                    ,header=True,inferSchema=True)\n",
    "    popularity_avg_perhour = gplus_feedback.select(header_per_hour)\\\n",
    "                            .rdd.map(list).flatMap(lambda x:((x[0], element) for element in x[1:]))\\\n",
    "                            .reduceByKey(add).map(lambda x:(x[0], x[1]/48)).sortByKey()\\\n",
    "                            .map(lambda x:('ID'+str(x[0]), x[1])).collect()\n",
    "    popularity_avg_perday = gplus_feedback.select(header_per_day).rdd.map(list)\\\n",
    "                            .flatMap(lambda x:((x[0], element) for element in x[1:]))\\\n",
    "                            .reduceByKey(add).map(lambda x:(x[0], x[1]/2)).sortByKey()\\\n",
    "                            .map(lambda x:('ID'+str(x[0]), x[1])).collect()\n",
    "    \n",
    "    if FileGPlus.split('_')[0] == SocMedia:\n",
    "        GooglePlus_per_hour = open('output/Q2/'+'GooglePlus_per_Hour.csv','w')\n",
    "        GooglePlus_per_day = open('output/Q2/'+'GooglePlus_per_Day.csv','w')\n",
    "        for j in popularity_avg_perhour:\n",
    "            k = ':'.join([str(l)for l in j])\n",
    "            GooglePlus_per_hour.write(k + '\\n')\n",
    "        for j in popularity_avg_perday:\n",
    "            k = ':'.join([str(l)for l in j])\n",
    "            GooglePlus_per_day.write(k + '\\n')\n",
    "        GooglePlus_per_hour.close()\n",
    "        GooglePlus_per_day.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feedback From LinkedIn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Social media LinkedIn\n",
    "SocMedia = 'LinkedIn'\n",
    "\n",
    "LinkedIn_per_Hour = open('output/Q2/'+'LinkedIn_per_Hour.csv','w')\n",
    "LinkedIn_per_Day = open('output/Q2/'+'LinkedIn_per_Day.csv','w')\n",
    "\n",
    "for FileLinkedIn in fileLinkedInList:\n",
    "    linkedin_feedback = spark.read.csv('/home/twster/Spark/Projects/datasets/hw2/'+FileLinkedIn+'.csv'\n",
    "                                       ,header=True,inferSchema=True)\n",
    "    popularity_avg_perhour = linkedin_feedback.select(header_per_hour)\\\n",
    "                            .rdd.map(list).flatMap(lambda x:((x[0], element) for element in x[1:]))\\\n",
    "                            .reduceByKey(add).map(lambda x:(x[0], x[1]/48)).sortByKey()\\\n",
    "                            .map(lambda x:('ID'+str(x[0]), x[1])).collect()\n",
    "    popularity_avg_perday = linkedin_feedback.select(header_per_day)\\\n",
    "                            .rdd.map(list).flatMap(lambda x:((x[0], element) for element in x[1:]))\\\n",
    "                            .reduceByKey(add).map(lambda x:(x[0], x[1]/2)).sortByKey()\\\n",
    "                            .map(lambda x:('ID'+str(x[0]), x[1])).collect()\n",
    "    \n",
    "    if FileLinkedIn.split('_')[0] == SocMedia:\n",
    "        LinkedIn_per_hour = open('output/Q2/'+'LinkedIn_per_Hour.csv','w')\n",
    "        LinkedIn_per_day = open('output/Q2/'+'LinkedIn_per_Day.csv','w')\n",
    "        for j in popularity_avg_perhour:\n",
    "            k = ':'.join([str(l)for l in j])\n",
    "            LinkedIn_per_hour.write(k + '\\n')\n",
    "        for j in popularity_avg_perday:\n",
    "            k = ':'.join([str(l)for l in j])\n",
    "            LinkedIn_per_day.write(k + '\\n')\n",
    "        LinkedIn_per_hour.close()\n",
    "        LinkedIn_per_day.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) In news data, calculate the sum and average sentiment score of each topic, respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary SentimentTitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SentimentScore = df4_news.withColumn(\"SentimentScore\", df4_news.SentimentTitle+df4_news.SentimentHeadline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SentimentScore.filter(df_SentimentScore['Topic'] == 'obama').groupBy('Topic')\\\n",
    "                    .agg(F.sum(\"SentimentScore\").alias(\"Summary\"))\\\n",
    "                    .coalesce(1).write.format(\"csv\").options(header='true')\\\n",
    "                    .save(\"output/Q3/\"+\"SentimentScore_Sum_Topic_obama\")\n",
    "            \n",
    "df_SentimentScore.filter(df_SentimentScore['Topic'] == 'economy').groupBy('Topic')\\\n",
    "                    .agg(F.sum(\"SentimentScore\").alias(\"Summary\"))\\\n",
    "                    .coalesce(1).write.format(\"csv\").options(header='true')\\\n",
    "                    .save(\"output/Q3/\"+\"SentimentScore_Sum_Topic_economy\")\n",
    "            \n",
    "df_SentimentScore.filter(df_SentimentScore['Topic'] == 'microsoft').groupBy('Topic')\\\n",
    "                    .agg(F.sum(\"SentimentScore\").alias(\"Summary\"))\\\n",
    "                    .coalesce(1).write.format(\"csv\").options(header='true')\\\n",
    "                    .save(\"output/Q3/\"+\"SentimentScore_Sum_Topic_microsoft\")\n",
    "                    \n",
    "df_SentimentScore.filter(df_SentimentScore['Topic'] == 'palestine').groupBy('Topic')\\\n",
    "                    .agg(F.sum(\"SentimentScore\").alias(\"Summary\"))\\\n",
    "                    .coalesce(1).write.format(\"csv\").options(header='true')\\\n",
    "                    .save(\"output/Q3/\"+\"SentimentScore_Sum_Topic_palestine\")\n",
    "            \n",
    "df_SentimentScore.filter(df_SentimentScore['Topic'] == 'obama').groupBy('Topic')\\\n",
    "                    .agg(F.mean(\"SentimentScore\").alias(\"Average\"))\\\n",
    "                    .coalesce(1).write.format(\"csv\").options(header='true')\\\n",
    "                    .save(\"output/Q3/\"+\"SentimentScore_Avg_Topic_obama\")\n",
    "    \n",
    "df_SentimentScore.filter(df_SentimentScore['Topic'] == 'economy').groupBy('Topic')\\\n",
    "                    .agg(F.mean(\"SentimentScore\").alias(\"Average\"))\\\n",
    "                    .coalesce(1).write.format(\"csv\").options(header='true')\\\n",
    "                    .save(\"output/Q3/\"+\"SentimentScore_Avg_Topic_economy\")\n",
    "    \n",
    "df_SentimentScore.filter(df_SentimentScore['Topic'] == 'microsoft').groupBy('Topic')\\\n",
    "                    .agg(F.mean(\"SentimentScore\").alias(\"Average\"))\\\n",
    "                    .coalesce(1).write.format(\"csv\").options(header='true')\\\n",
    "                    .save(\"output/Q3/\"+\"SentimentScore_Avg_Topic_microsoft\")\n",
    "    \n",
    "df_SentimentScore.filter(df_SentimentScore['Topic'] == 'palestine').groupBy('Topic')\\\n",
    "                    .agg(F.mean(\"SentimentScore\").alias(\"Average\"))\\\n",
    "                    .coalesce(1).write.format(\"csv\").options(header='true')\\\n",
    "                    .save(\"output/Q3/\"+\"SentimentScore_Avg_Topic_palestine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) From subtask (1), for the top-100 frequent words per topic in titles and headlines, calculate their co-occurrence matrices (100x100), respectively. Each entry in the matrix will contain the co-occurrence frequency in all news titles and headlines, respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Obama in Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic_obama_in_title\n",
    "TTO = TitleWords.groupBy(\"Title\",\"Topic\").count()\\\n",
    "        .orderBy(desc(\"count\")).where(TitleWords[\"Topic\"]==\"obama\")\\\n",
    "        .withColumnRenamed(\"Title\",\"Word\")        \n",
    "TopWordObama = TTO.select(\"Word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "TTO2 = df4_news.select(\"Title\",\"Topic\").filter(df4_news[\"Topic\"]==\"obama\")\n",
    "TTO3 = TTO2.select(\"Title\")\n",
    "\n",
    "#convert DF to RDD\n",
    "title_obama = TTO3.rdd.flatMap(lambda x: x).collect()\n",
    "topic_obama_title = TopWordObama.rdd.flatMap(lambda x: x).take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occurrence_matrices=[[0]*100 for i in range(100)]\n",
    "out=open('output/Q4/'+'obama_matrix_title.txt','w')\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        for k in range(len(title_obama)):\n",
    "            if topic_obama_title[i] in title_obama[k] and topic_obama_title[j] in title_obama[k]:\n",
    "                co_occurrence_matrices[i][j]+=1\n",
    "        out.write(str(co_occurrence_matrices[i]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic economy in Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "TTE = TitleWords.groupBy(\"Title\",\"Topic\").count()\\\n",
    "        .orderBy(desc(\"count\")).where(TitleWords[\"Topic\"]==\"economy\")\\\n",
    "        .withColumnRenamed(\"Title\",\"Word\")        \n",
    "TopWordEconomy = TTE.select(\"Word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "TTE2 = df4_news.select(\"Title\",\"Topic\").filter(df4_news[\"Topic\"]==\"economy\")\n",
    "TTE3 = TTE2.select(\"Title\")\n",
    "\n",
    "title_economy = TTE3.rdd.flatMap(lambda x: x).collect()\n",
    "topic_economy_title = TopWordEconomy.rdd.flatMap(lambda x: x).take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occurrence_matrices=[[0]*100 for i in range(100)]\n",
    "out=open('output/Q4/'+'economy_matrix_title.txt','w')\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        for k in range(len(title_economy)):\n",
    "            if topic_economy_title[i] in title_economy[k] and topic_economy_title[j] in title_economy[k]:\n",
    "                co_occurrence_matrices[i][j]+=1\n",
    "        out.write(str(co_occurrence_matrices[i]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic microsoft in Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "TTM = TitleWords.groupBy(\"Title\",\"Topic\").count()\\\n",
    "        .orderBy(desc(\"count\")).where(TitleWords[\"Topic\"]==\"microsoft\")\\\n",
    "        .withColumnRenamed(\"Title\",\"Word\")        \n",
    "TopWordMicrosoft = TTM.select(\"Word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "TTM2 = df4_news.select(\"Title\",\"Topic\").filter(df4_news[\"Topic\"]==\"microsoft\")\n",
    "TTM3 = TTM2.select(\"Title\")\n",
    "\n",
    "#convert DF to RDD\n",
    "title_microsoft = TTM3.rdd.flatMap(lambda x: x).collect()\n",
    "topic_microsoft_title = TopWordMicrosoft.rdd.flatMap(lambda x: x).take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occurrence_matrices=[[0]*100 for i in range(100)]\n",
    "out=open('output/Q4/'+'microsoft_matrix_title.txt','w')\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        for k in range(len(title_microsoft)):\n",
    "            if topic_microsoft_title[i] in title_microsoft[k] and topic_microsoft_title[j] in title_microsoft[k]:\n",
    "                co_occurrence_matrices[i][j]+=1\n",
    "        out.write(str(co_occurrence_matrices[i]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic palestine in Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "TTP = TitleWords.groupBy(\"Title\",\"Topic\").count()\\\n",
    "        .orderBy(desc(\"count\")).where(TitleWords[\"Topic\"]==\"palestine\")\\\n",
    "        .withColumnRenamed(\"Title\",\"Word\")        \n",
    "TopWordPalestine = TTP.select(\"Word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "TTP2 = df4_news.select(\"Title\",\"Topic\").filter(df4_news[\"Topic\"]==\"palestine\")\n",
    "TTP3 = TTP2.select(\"Title\")\n",
    "\n",
    "title_palestine = TTP3.rdd.flatMap(lambda x: x).collect()\n",
    "topic_palestine_title = TopWordPalestine.rdd.flatMap(lambda x: x).take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occurrence_matrices=[[0]*100 for i in range(100)]\n",
    "out=open('output/Q4/'+'palestine_matrix_title.txt','w')\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        for k in range(len(title_palestine)):\n",
    "            if topic_palestine_title[i] in title_palestine[k] and topic_palestine_title[j] in title_palestine[k]:\n",
    "                co_occurrence_matrices[i][j]+=1\n",
    "        out.write(str(co_occurrence_matrices[i]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Obama in Headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTO = HeadlineWords.groupBy(\"Headline\",\"Topic\").count()\\\n",
    "        .orderBy(desc(\"count\")).where(HeadlineWords[\"Topic\"]==\"obama\")\\\n",
    "        .withColumnRenamed(\"Headline\",\"Word\")        \n",
    "TopWordObamaHL = HTO.select(\"Word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTO2 = df4_news.select(\"Headline\",\"Topic\").filter(df4_news[\"Topic\"]==\"obama\")\n",
    "HTO3 = HTO2.select(\"Headline\")\n",
    "\n",
    "headline_obama=HTO3.rdd.flatMap(lambda x: x).filter(lambda x:type(x)==str).collect()\n",
    "topic_obama_headline = TopWordObamaHL.rdd.flatMap(lambda x: x).take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occurrence_matrix=[[0]*100 for i in range(100)]\n",
    "output=open('output/Q4/'+'obama_matrix_headline.txt','w')\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        for k in range(len(headline_obama)):\n",
    "            if topic_obama_headline[i] in headline_obama[k] and topic_obama_headline[j] in headline_obama[k]:\n",
    "                co_occurrence_matrix[i][j]+=1\n",
    "        output.write(str(co_occurrence_matrix[i]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Economy in Headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTE = HeadlineWords.groupBy(\"Headline\",\"Topic\").count()\\\n",
    "        .orderBy(desc(\"count\")).where(HeadlineWords[\"Topic\"]==\"economy\")\\\n",
    "        .withColumnRenamed(\"Headline\",\"Word\")\n",
    "TopWordEconomyHL = HTE.select(\"Word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTE2 = df4_news.select(\"Headline\",\"Topic\").filter(df4_news[\"Topic\"]==\"economy\")\n",
    "HTE3 = HTE2.select(\"Headline\")\n",
    "\n",
    "headline_economy = HTE3.rdd.flatMap(lambda x: x).filter(lambda x:type(x)==str).collect()\n",
    "topic_economy_headline = TopWordEconomyHL.rdd.flatMap(lambda x: x).take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occurrence_matrix=[[0]*100 for i in range(100)]\n",
    "output=open('output/Q4/'+'economy_matrix_headline.txt','w')\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        for k in range(len(headline_economy)):\n",
    "            if topic_economy_headline[i] in headline_economy[k] and topic_economy_headline[j] in headline_economy[k]:\n",
    "                co_occurrence_matrix[i][j]+=1\n",
    "        output.write(str(co_occurrence_matrix[i]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Microsoft in Headline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTM = HeadlineWords.groupBy(\"Headline\",\"Topic\").count()\\\n",
    "        .orderBy(desc(\"count\")).where(HeadlineWords[\"Topic\"]==\"microsoft\")\\\n",
    "        .withColumnRenamed(\"Headline\",\"Word\")\n",
    "TopWordMicrosoftHL = HTM.select(\"Word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTM2 = df4_news.select(\"Headline\",\"Topic\").filter(df4_news[\"Topic\"]==\"microsoft\")\n",
    "HTM3 = HTM2.select(\"Headline\")\n",
    "\n",
    "headline_microsoft = HTM3.rdd.flatMap(lambda x: x).filter(lambda x:type(x)==str).collect()\n",
    "topic_microsoft_headline = TopWordMicrosoftHL.rdd.flatMap(lambda x: x).take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occurrence_matrix=[[0]*100 for i in range(100)]\n",
    "output=open('output/Q4/'+'microsoft_matrix_headline.txt','w')\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        for k in range(len(headline_microsoft)):\n",
    "            if topic_microsoft_headline[i] in headline_microsoft[k] and topic_microsoft_headline[j] in headline_microsoft[k]:\n",
    "                co_occurrence_matrix[i][j]+=1\n",
    "        output.write(str(co_occurrence_matrix[i]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Palestine in Headline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTP = HeadlineWords.groupBy(\"Headline\",\"Topic\").count()\\\n",
    "        .orderBy(desc(\"count\")).where(HeadlineWords[\"Topic\"]==\"palestine\")\\\n",
    "        .withColumnRenamed(\"Headline\",\"Word\")\n",
    "TopWordPalestineHL = HTP.select(\"Word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTP2 = df4_news.select(\"Headline\",\"Topic\").filter(df4_news[\"Topic\"]==\"'palestine'\")\n",
    "HTP3 = HTP2.select(\"Headline\")\n",
    "\n",
    "headline_palestine = HTM3.rdd.flatMap(lambda x: x).filter(lambda x:type(x)==str).collect()\n",
    "topic_palestine_headline = TopWordPalestineHL.rdd.flatMap(lambda x: x).take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occurrence_matrix=[[0]*100 for i in range(100)]\n",
    "output=open('output/Q4/'+'palestine_matrix_headline.txt','w')\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        for k in range(len(headline_palestine)):\n",
    "            if topic_palestine_headline[i] in headline_palestine[k] and topic_palestine_headline[j] in headline_palestine[k]:\n",
    "                co_occurrence_matrix[i][j]+=1\n",
    "        output.write(str(co_occurrence_matrix[i]) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
